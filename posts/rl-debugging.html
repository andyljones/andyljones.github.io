<!doctype HTML>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Andy Jones">
    <meta name="format-detection" content="telephone=no">

    <link rel="icon" href="/favicon.ico">

    <style>
      :root {
        --tone: #6d2e98;
        --grey: #444;
        --white: #f8f8f8;
      }

      html {
          height: 100%;
          width: 100%;
      }

      body {
          display: flex;
          flex-direction: column;
          background: var(--tone);
          color: #F8F8F8;
          width: 100%;
          font-family: sans-serif;
          margin: 0px;
          text-align: justify;
      }

      a:visited {
          color: var(--tone);
      }

      .tinted {
          color: #444;
          background: #f8f8f8;
          text-decoration: none;
      }

      #banner a {
          color: #F8F8F8;
          text-decoration: none;
      }

      #footer a {
          color: #F8F8F8;
          text-decoration: none;
      }

      .column {
          max-width: min(100%, 960px);
          margin: auto;
          padding: 5px;
      }

      #banner {
          padding: 10px;
          font-size: large;
          display: flex;
          vertical-align: middle;
          justify-content: space-between;
      }

      #title {
          font-weight: 700;
      }

      #social img {
          filter: invert();
          height: 20px;
          width: 20px;
          padding-left: 2px;
          padding-right: 2px;
      }

      .sep {
          margin-left: 10px;
          margin-right: 10px;
      }


      #content {
          margin-bottom: 5px;
          margin: auto;
      }

      #date {
          margin: auto;
      }

      code {
          font-family: "Lucida Console", monospace;
          word-wrap: break-word;
      }

      pre {
          font-family: "Lucida Console", monospace;
          background-color: #EEE;
          padding: 10px;

          white-space: pre-wrap;
          word-wrap: break-word;
      }

      sup {
          font-size: x-small;
      }

      hr {
          border-color: var(--tone);
      }

      #content img {
          display: block;
          margin: auto;
          max-width: 100%;
      }

      #footer {
          padding: 5px;
          font-size: x-small;
          vertical-align: middle;
          margin: auto;
          text-align: center;
      }

    .hll { background-color: #ffffcc }
.c { color: #408080; font-style: italic } /* Comment */
.err { border: 1px solid #FF0000 } /* Error */
.k { color: #008000; font-weight: bold } /* Keyword */
.o { color: #666666 } /* Operator */
.ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.cm { color: #408080; font-style: italic } /* Comment.Multiline */
.cp { color: #BC7A00 } /* Comment.Preproc */
.cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.c1 { color: #408080; font-style: italic } /* Comment.Single */
.cs { color: #408080; font-style: italic } /* Comment.Special */
.gd { color: #A00000 } /* Generic.Deleted */
.ge { font-style: italic } /* Generic.Emph */
.gr { color: #FF0000 } /* Generic.Error */
.gh { color: #000080; font-weight: bold } /* Generic.Heading */
.gi { color: #00A000 } /* Generic.Inserted */
.go { color: #888888 } /* Generic.Output */
.gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.gs { font-weight: bold } /* Generic.Strong */
.gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.gt { color: #0044DD } /* Generic.Traceback */
.kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.kp { color: #008000 } /* Keyword.Pseudo */
.kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.kt { color: #B00040 } /* Keyword.Type */
.m { color: #666666 } /* Literal.Number */
.s { color: #BA2121 } /* Literal.String */
.na { color: #7D9029 } /* Name.Attribute */
.nb { color: #008000 } /* Name.Builtin */
.nc { color: #0000FF; font-weight: bold } /* Name.Class */
.no { color: #880000 } /* Name.Constant */
.nd { color: #AA22FF } /* Name.Decorator */
.ni { color: #999999; font-weight: bold } /* Name.Entity */
.ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.nf { color: #0000FF } /* Name.Function */
.nl { color: #A0A000 } /* Name.Label */
.nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.nt { color: #008000; font-weight: bold } /* Name.Tag */
.nv { color: #19177C } /* Name.Variable */
.ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.w { color: #bbbbbb } /* Text.Whitespace */
.mb { color: #666666 } /* Literal.Number.Bin */
.mf { color: #666666 } /* Literal.Number.Float */
.mh { color: #666666 } /* Literal.Number.Hex */
.mi { color: #666666 } /* Literal.Number.Integer */
.mo { color: #666666 } /* Literal.Number.Oct */
.sa { color: #BA2121 } /* Literal.String.Affix */
.sb { color: #BA2121 } /* Literal.String.Backtick */
.sc { color: #BA2121 } /* Literal.String.Char */
.dl { color: #BA2121 } /* Literal.String.Delimiter */
.sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.s2 { color: #BA2121 } /* Literal.String.Double */
.se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.sh { color: #BA2121 } /* Literal.String.Heredoc */
.si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.sx { color: #008000 } /* Literal.String.Other */
.sr { color: #BB6688 } /* Literal.String.Regex */
.s1 { color: #BA2121 } /* Literal.String.Single */
.ss { color: #19177C } /* Literal.String.Symbol */
.bp { color: #008000 } /* Name.Builtin.Pseudo */
.fm { color: #0000FF } /* Name.Function.Magic */
.vc { color: #19177C } /* Name.Variable.Class */
.vg { color: #19177C } /* Name.Variable.Global */
.vi { color: #19177C } /* Name.Variable.Instance */
.vm { color: #19177C } /* Name.Variable.Magic */
.il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>

    <meta property="og:title" content="Debugging Reinforcement Learning Systems">
    <meta property="og:description" content="How to debug your reinforcement learning implementations, without the agonizing pain">
    <meta property="og:image" content="https://andyljones.com/icons/robot-solid.png">
    <meta property="og:image:height" content="512">
    <meta property="og:image:width" content="512">
    
    <meta name="twitter:card" content="summary">

    <title>Debugging Reinforcement Learning Systems</title>

    <!-- Monitoring pageviews is really useful to me to see what kind of stuff I've done is useful to everyone else.
    But I'm keen to not to track any more than that, and so this is a minimal, self-hosted solution. It records
    nothing more than'd be in the server logs anyway. -->
    <script>
      url = "https://live.andyljones.com/mat/mat.php";
      xhr = new XMLHttpRequest();
      xhr.open("POST", url, true);
      xhr.setRequestHeader('Content-Type', 'application/x-www-form-urlencoded');

      params = 'idsite=1&rec=1'
      params += '&url=' + encodeURIComponent(window.location.href);
      params += '&urlref=' + encodeURIComponent(document.referrer);
      params += '&rand=' + Math.floor(16384*Math.random())
      xhr.send(params)
    </script>
    <noscript>
      <img src="https://live.andyljones.com/mat/mat.php?idsite=1&amp;rec=1" style="border:0" alt=""/>
    </noscript>

  </head>
  <body>
    <div>
      <div id="banner" class="column">
        <div id="title"><a href="/">andy jones</a></div>
        <div id="social">
          <a href="/rss.xml"><img src='/icons/rss-solid.svg' alt='RSS'></a>
          <span class='sep'></span>
          <a href="mailto:me@andyljones.com"><img src='/icons/at-solid.svg' alt='Email'></a>
          <span class='sep'></span>
          <a href="https://github.com/andyljones"><img src='/icons/github-brands.svg' alt='Github'></a>
          <a href="https://www.linkedin.com/in/andyjonescs"><img src='/icons/linkedin-in-brands.svg' alt='LinkedIn'></a>
          <span class='sep'></span>
          <a href="https://twitter.com/andy_l_jones"><img src='/icons/twitter-brands.svg' alt='Twitter'></a>
          <a href="https://www.reddit.com/u/bluecoffee"><img src='/icons/reddit-brands.svg' alt='Reddit'></a>
          <a href="https://stackoverflow.com/users/2565457/andy-jones"><img src='/icons/stack-overflow-brands.svg' alt='StackOverflow'></a>
        </div>
      </div>
    </div>

    <div class="tinted">
      <div id="content" class="column">
        <p><strong>This is still a draft. If you come across it somehow, please do not share it until it's complete</strong></p>
<p>Debugging reinforcement learning algorithms is extremely hard. You might think 'yes, but I'm very smart!'. Well, <a href="https://news.ycombinator.com/item?id=13519044">here's the (now) head of Tesla's AI division taking six weeks to write a from-scratch policy gradients implementation, despite having all of OpenAI's expertise available to him</a>.</p>
<p>I am not as good a machine learning researcher as Andrej Karpathy, and I say with  confidence you aren't either.</p>
<p>You should expect to spend orders of magnitude longer debugging and validating your algorithm than you do writing it in the first place. If this is your first time, you might plausibly have a few hundred lines of code that you <em>think</em> are correct in an hour, and a system that's <em>actually</em> correct two months later.</p>
<h1>Concrete Advice</h1>
<p>With that in mind, let's discuss some ways to make the misery of debugging less miserable. I've put this advice in priority order.</p>
<h2>1. Work from a reference implementation</h2>
<p><em>If you're new to reinforcement learning, writing things from scratch is the most catastrophically self-sabotaging thing you can do.</em></p>
<p>There is an alluring masochism in writing things from scratch. There's concrete value in it too: by writing things from scratch, you're both forced to fully understand what you're doing and you're more likely to come up with a fresh perspective. In many other fields of software development these benefits would be worth the slow-down you suffer from having to work everything out yourself.</p>
<p>In reinforcement learning, these benefits are not worth it. At all. As discussed below, the nature of RL work makes it extremely hard for you to self-correct.</p>
<p>When I say 'use a reference implementation', there are several interpretations you can take depending on your risk tolerance.</p>
<ul>
<li>The safest thing to do is to use a reference implementation out-of-the-box. Check that it works on your task, then repeatedly make a small change and check that it still works. </li>
<li>Less safe is to just use the reference implementation as a source of reliable components. Work to the same API, and check that giving your version of a component and their version give the same outputs.</li>
<li>Least safe (but still dramatically better than going in blind) is to have one eye on the reference implementation while you write your own. Copy their hyperparameters, copy their discounting code, copy how they handle termination and invalid actions and a hundred other little things that you're likely to muck up otherwise. </li>
</ul>
<p>Here are some excellent reference implementations to choose from:</p>
<ul>
<li><a href="https://github.com/openai/spinningup">spinning-up</a> has been written by OpenAI, and has a <a href="https://spinningup.openai.com/">short course to go along with it</a>.</li>
<li><a href="https://github.com/DLR-RM/stable-baselines3">stable-baselines3</a> is based on an older set of OpenAI implementations, but cleaned up and actively maintained.</li>
<li><a href="https://github.com/vwxyzjn/cleanrl/tree/master/cleanrl">cleanrl</a> isolates every algorithm in its own file.</li>
<li><a href="https://github.com/deepmind/open_spiel">OpenSpiel</a> is DeepMind's multi-agent reinforcement learning library. They provide both Python and C++ implementations of many algorithms - you'll probably want the Python ones.</li>
</ul>
<h2>2. Assume you have a bug</h2>
<p>When their RL implementation doesn't work, people are often keen to either (a) adjust their network architecture or (b) adjust their hyperparameters. On the other hand, they're reluctant to say they've got a bug.</p>
<p>Most often, it turns out they've got a bug.</p>
<p>Why bugs are so much more common in RL code is [[RL Debugging Without the Agonizing Pain#Your Intuition Sucks|discussed below]], but there's another advantage to assuming you've got a bug: bugs are a damn sight faster to find and fix than validating that your new architecture is an improvement over the old one.</p>
<p>Now having said that you should assume you have a bug, it's worth mentioning that sometimes - rarely - you don't. What I'm advocating for here is not a blind faith in the buginess of your code, but for dramatically raising the threshold at which you start thinking 'OK, I think this is correct.'</p>
<h2>3. Stop looking at your loss curves</h2>
<p>When someone's RL implementation isn't working, they <em>luuuuuurv</em> to copy-paste a screenshot of their loss curve to you. They do this because they know they want a pretty, exponentially-decaying loss curve, and they know what they have <em>isn't that</em>.</p>
<p>The problem with using the loss curve as an indicator of correctness is somewhat that it's not reliable, but mostly because it doesn't localise errors. The shape of your loss curve says very little about where in your code you've messed up, and so says very little about what you need to change to get things working.</p>
<p>As in the previous section, my sweeping proclamation comes with some qualifiers. Once you have a semi-functional implementation and you've exhausted other, better methods of error localisation (as documented in the rest of this post), there <em>is</em> valuable information in a loss curve.</p>
<p>If nothing else, being able to split a model's performance into 'how fast it learns' and 'where it plateaus' is a useful way to think about the next improvement you might want to make. But because it only offers <em>global</em> information about the performance of your implementation, it makes for a really shitty debugging tool.</p>
<h2>4. Test out the tricky bits</h2>
<p>Most of the bugs in a typical attempt at an RL implementation turn up in the same few places. Some of the usual suspects are</p>
<ul>
<li>reward discounting, especially around episode resets</li>
<li>advantage calculations, again especially around resets</li>
<li>buffering and batching, especially pairing the wrong rewards with the wrong observations</li>
</ul>
<p>Fortunately, these components are all really easy to test! They've got none of the issues that validating RL algorithms as a whole has. These components are deterministic, they're easy to factor out, and they're fast. Checking you've got the termination right on your reward discounting is <a href="https://github.com/andyljones/megastep/blob/master/megastep/demo/learning.py#L134-L159">a few lines</a>.</p>
<p>What's even better is that most of the time, <em>as you write these things</em> you know you're messing them up. If you're not certain whether you've just accumulated the reward on one side of the reset or the other, <em>put a test in</em>.</p>
<h2>5. Use simpler environments. No, even simpler than that.</h2>
<p>The usual advice to people writing RL algorithms is to use a simple environment like the <a href="https://gym.openai.com/envs/#classic_control">classic control ones from the Gym</a>.</p>
<p>Thing is, these envs have the same problem as looking at loss curves: at best they give you a noisy indicator, and if the noisy indicator looks poor you don't know <em>why</em> it looks poor. They don't localise errors.</p>
<p>Instead, construct environments that <em>do</em> localise errors. In a recent project, I used</p>
<ol>
<li><strong>One action, zero observation, one timestep long, +1 reward every timestep</strong>: This isolates the value network. If my agent can't learn that the value of the only observation it ever sees it 1, there's a problem with the value loss calculation or the optimizer.</li>
<li><strong>One action, zero observation, <em>two</em> timesteps long, +1 reward at the end</strong>: If my agent can learn the value in (1.) but not this one, it must be that my reward discounting is broken.</li>
<li><strong>One action, random +1/-1 observation, one timestep long, obs-dependent +1/-1 reward every time</strong>: If my agent can learn the value in (1.) but not this one - meaning it can learn a constant reward but not a predictable one! - it must be that backpropagation through my network is broken.</li>
<li><strong>Two actions, zero observation, one timestep long, action-dependent +1/-1 reward</strong>: The first env to exercise the policy! If my agent can't learn to pick the better action, there's something wrong with either my advantage calculations, my policy loss or my policy update. That's three things, but it's easy to work out by hand the expected values for each one and check that the values produced by your actual code line up with them.</li>
<li><strong>Two actions, random +1/-1 observation, one timestep long, action-and-obs dependent +1/-1 reward</strong>: Now we've got a dependence on both obs and action. The policy and value networks interact here, so there's a couple of things to verify: that the policy network learns to pick the right action in each of the two states, and that the value network learns that the value of each state is +1. If everything's worked up until now, then if - for example - the value network fails to learn here, it likely means your batching process is feeding the value network stale experience.</li>
<li>Etc.</li>
</ol>
<p>You get the idea: (1.) is the simplest possible environment, and each new env adds the smallest possible bit of functionality. If the old env works but the successor doesn't, that gives you a <em>lot</em> of information about where the problem is.</p>
<p>Even better, these environments are extraordinarily fast. When you've a correct implementation, it should only take a second or two to learn them. And they're <em>decisive</em>: if your value network in (1.) ends up more than an epsilon away from the correct value, it means you've got a bug.</p>
<p>As an aside, if you find yourself switching out envs a lot it makes sense to write your network with swappable 'heads': pass the <code>obs_space</code> and <code>action_space</code> of the env to your network's initializer, and let it nail on</p>
<ul>
<li>an intake that'll  transform samples from the obs space to flat vectors for your network to digest,</li>
<li>an output that'll transform flat vectors from your network into the outputs the env expects.</li>
</ul>
<p>This is an idea that's been developed a few times independently, though I can't remember where else I've seen it just this second. You can find my own (undocumented) implementation <a href="https://github.com/andyljones/megastep/blob/23347dbc4698626408e4c5047c9f5b0a803c4e72/megastep/demo/heads.py#L69-L75">here</a>.</p>
<h2>6. Use simple agents. No, even simpler than that.</h2>
<p>In much the same way that you can simplify your environments to localise errors, you can do the same with your agents too.</p>
<ul>
<li>Cheats</li>
<li>Automatons</li>
<li>Tabular</li>
</ul>
<p>TODO: More of this.</p>
<h2>7. Log <em>everything</em></h2>
<p>The last three sections have involved controlled experiments of a sort, where you place your components in a known setup and see how they act. The compliment to a controlled experiment is an observational study: watching your system in its natural habitat <em>very carefully</em> and seeing if you can spot anything anomalous.</p>
<p>In reinforcement learning, watching your system carefully means logging. Lots of logging. Logs that I've found particularly useful are</p>
<h3>Relative policy entropy</h3>
<p>The entropy of your policy network's outputs, relative to the maximum possible entropy. It'll usually start near 1, then rapidly fall for a while, then flatten out for the rest of training.</p>
<p>If it stays very near 1, your agent is failing to learn any policy at all. You should check that your policy targets are being computed correctly, that the gradient's being backpropagated correctly, and - if you've defined a custom environment - then your environment is actually correct!</p>
<p>If it drops to zero or close to zero, then your agent has 'collapsed' into some - likely myopic - policy, and isn't exploring any more. This is usually because you'v either forgotten to include an exploration mechanism of some sort (like epsilon-greedy actions or an entropy term in the loss), or because your rewards are much larger than whatever you're using to encourage exploration.</p>
<p>Sometimes it'll go up for a while; don't stress about that unless it's a large, permanent increase. If it <em>is</em> a large permanent increase and the minimum was very early in training, that can be an indicator that your policy fell into some myopic obviously-good behaviour that it's having to gradually climb back out of. It might help to turn up the exploration incentives.</p>
<p>If the entropy oscillates wildly, that usually means your learning rate is too high.</p>
<h3>Kullback-Leibler divergence</h3>
<p>The KL div between the policy that was used to collect the experience in the batch, and the policy that your learner's just generated for the same batch. This should be small but positive.</p>
<p>If it's very large then your agent is having to learn from experience that's very different to the current policy. In some algorithms - like those with a replay buffer - that's expected, and all that's important is the KL div is stable. In other algorithms (like PPO), a very large KL div is an indicator that the experience reaching your network is 'stale', and that'll slow down training.</p>
<p>If it's very low then that suggests your network hasn't changed much in the time since the experience was generated, and you can probably get away with turning the learning rate up.</p>
<p>If it's growing steadily over time, that means you're probably feeding the same experience from early on in training back into the network again and again. Check your buffering system.</p>
<p>If it's negative - that shouldn't happen, and it means you're likely calculating the KL div incorrectly (probably by not handling invalid actions).</p>
<h3>Residual variance</h3>
<p>The variance of (target values - network values), divided by the variance of the target values.</p>
<p>Like the policy entropy, this should start close to 1, fall very rapidly early on, and then decrease more gradually over the course of training.</p>
<p>If it stays near 1, your value network isn't learning to predict the rewards. Check that your rewards are what you think they are, and check that your value loss and backprop through the value net are all working correctly.</p>
<p>If it drops to zero, that's usually because the policy entropy has dropped to zero too, the policy has collapsed into some deterministic behaviour, and the value network has learned the rewards it is collecting perfectly. Another common reason is that some scenarios are generating vastly larger returns than the others, and the value net's learned to identify when that happens.</p>
<p>If the residual variance oscillates wildly, that usually means your learning rate is too high.</p>
<h3>Terminal correlation</h3>
<p>The correlation between the value in the final state and the reward in the final step. This is only useful when there's lots of reward in the final step (like in boardgames).</p>
<p>It should start near zero, rise rapidly, then plateau near 1.</p>
<p>If it stays near zero but all the other value-related logs look good, then check that your reward-to-gos are being calculated correctly near termination!</p>
<p>If reward is more evenly distributed through the episode, you could write a version of this that looks at the correlation of (next state's value - this state's value) with the reward in that step. I haven't used this myself though, so can't offer commentary.</p>
<h3>Penultimate terminal correlation</h3>
<p>The correlation between the value in the penultimate step and the final reward. Again, only useful when there's lots of reward at the end of the episode. If terminal correlation is high but penultimate terminal correlation is low, that's a strong indicator that your reward-to-gos aren't being carried backwards properly.</p>
<h3>Value target distribution</h3>
<p>Either plot a histogram, or the min/max/mean/std. The plots should indicate 'reasonable' value targets in the range [-10, +10] (and ideally  [-3, +3]).</p>
<p>If they're larger than that, make your rewards proportionately smaller; if they're  smaller than that, make your rewards larger.</p>
<p>If they blow up, check that your reward discounting is correct, and possibly make your discount rate smaller.</p>
<p>If they're blowing up but you're insistent on leaving the discount rate where it is, one alternative is to increase the number of steps used to bootstrap the value targets. In PPO, this'd mean using longer chunks. Longer chunks mean that the values used for bootstrapping get shrunk more before they're fed back to the value net as targets, increasing the stability. You could also consider annealing the discount factor from a smaller value up towards 1.</p>
<h3>Reward distribution</h3>
<p>Again, as a histogram or min/max/mean/std. What a reasonable reward distribution is depends on the environment; some envs have a few large rewards, while others have lots of small rewards. Either way, if it doesn't match your expectations then you should investigate.</p>
<h3>Value distribution</h3>
<p>Again, as a histogram or min/max/mean/std. This is a complement to the previous two distributions and <em>should</em> closely match the value target distribution. If it doesn't, and it stays different from the value target distribution, that's an indicator that your value network is having trouble learning.</p>
<p>It's also worth keeping an eye on the sign of the distribution. If your env only produces positive rewards but there are persistently negatives values in the value target distribution, that suggests your reward-to-go mechanism is badly broken or your value network is failing to learn.</p>
<h3>Advantage distribution</h3>
<p>Again, as a histogram or min/max/mean/std. As with the value targets, these should be in the range [-10, +10] (and ideally [-3, +3]).</p>
<p>Advantages should also be approximately mean-zero due to how they're constructed; if they're persistently not then you've messed up your advantage calculations.</p>
<h3>Episode length distribution</h3>
<p>Again, as a histogram or a min/max/mean/std. As with the reward distribution, interpreting this depends on the environment. If your environment should have arbitrary-length episodes, but you're seeing that every episode here is length 7, that indicates your environment is broken or your network's fallen into some degenerate behaviour.</p>
<h3>Sample staleness</h3>
<p>Sample staleness is the number of learner steps between the network used to generate a sample, and the network currently learning from that sample. You can generate this by setting an 'age' attribute on the network, and incrementing it at every learner step. Then when a sample arrives at the learner, diff it against the learner's current age.</p>
<p>How to interpret this depends on the algorithm, but it should generally stay at a steady value throughout training. In on-policy algorithms, lower sample stalenesses are better; in off-policy algorithms it's a tradeoff between fresh samples that let the network bootstrap quickly, and aged samples that stabilise things.</p>
<h3>Step statistics</h3>
<p>Step statistics are the abs-max and mean-square-value of the difference between the network's parameters when it enters the learner, and the network's parameters when it leaves the learner.</p>
<p>Interpreting this depends on a whole bunch of things, but the mean-square value should typically be very small (1e-3 in my current training run with a LR of 1e-2), while the abs-max should small yet substantially larger than the mean-square-value.</p>
<p>If the statistics are much smaller than that, you might be able to increase your learning rate; if they're much larger than that then be on the lookout for instability in your training.</p>
<h3>Gradient statistics</h3>
<p>Gradient statistics re the abs-max and mean-square-value of the gradient. In the age of Adam and other quasi-Newton optimizers, this isn't as informative as it once was, because normalising by the curvature estimates can dramatically inflate or collapse the gradient.</p>
<p>That said, if the step statistics are looking strange, this can help diagnose whether the problem is with the gradient calculation or with Adam's second-order magic.</p>
<h3>Gradient noise</h3>
<p>This is from <a href="https://arxiv.org/abs/1812.06162">McCandlish and Kaplan</a>, and it's intended to help you choose your batch size. Unfortunately it's <em>spectacularly</em> noisy, to the point where you likely want to average over all steps in your run.</p>
<p>I've been thinking that it might be possible to get more stable estimates of the gradient noise from Adam's moment estimates, but that's decidedly on the to-do list.</p>
<h3>Component throughput</h3>
<p>At the least, the actor throughput and learner in terms of samples per second and steps per second.</p>
<p>Typically the actor should be generating <em>at most</em> as many samples as the learner is consuming. If the actor is generating excess samples there are weak reasons that might be a good thing - it'll refresh the replay buffer more rapidly - but typically it's considered a waste of compute.</p>
<p>More generally, you want to see these remain stable throughout training. If they gradually decay, you're accumulating some costly state somewhere in your system.</p>
<p>(For me, problems with gradually-slowing-down systems have always turned out to be with stats and logging, but I suspect that's because I've rolled my own stats and logging systems)</p>
<h3>Value trace</h3>
<p>The trace of the value over a random episode from recent history, plotted together with the rewards. This can be useful if you suspect your value function or rewards of 'being weird' in some way; the value trace should typically be a collection of exponentially-increasing curves leading up to rewards, followed by vertical drops as the agent collects those rewards.</p>
<h3>GPU stats</h3>
<p>There are several GPU-related stats that are worth tracking. First are the memory stats, which in PyTorch include</p>
<ul>
<li>the <em>memory allocation</em>, as reported by <code>torch.cuda.max_memory_allocated</code>. This is how much memory has actually been <em>used</em> by your computations,</li>
<li>the <em>memory reserve</em>, as reported by <code>torch.cuda.max_memory_reserved</code>. This is how much memory PyTorch has <em>set aside</em> for your computations,</li>
<li>the <em>memory gross</em>, as reported by <code>nvidia-smi</code>. This is how much memory PyTorch is using overall, <a href="https://github.com/pytorch/pytorch/issues/20532#issuecomment-540628939">including the ~gigabyte it needs for its own kernels</a>. It's this figure that'll crash your program if it hits the GPU's memory limit.</li>
</ul>
<p>Keeping track of all three is useful for diagnosing memory issues: figuring out if it's you that's hanging onto too many tensors, or PyTorch that's being too aggressive with its caching.</p>
<p>If you're running out of memory and you can't immediately figure out why, <a href="https://github.com/Stonesjtu/pytorch_memlab#memory-profiler">memlab</a> can help a lot. Disclosure: I wrote the frontend.</p>
<p>As well as the memory stats, it's also useful to track the utilization, fan speed and temperature reported by <code>nvidia-smi</code>. You can get these values in <a href="https://github.com/andyljones/megastep/blob/master/rebar/stats/gpu.py#L17-L29">machine-readable form</a>.</p>
<p>In particular, if the utilization is persistently low then you should profile your code. Make sure to set <code>CUDA_LAUNCH_BLOCKING=1</code> before importing your tensor library, and then use <a href="https://jiffyclub.github.io/snakeviz/">snakeviz</a> or <a href="https://github.com/nschloe/tuna">tuna</a> to profile things in a broad way. If that's not enough detail, you can dig into things further with <a href="https://developer.nvidia.com/nsight-systems">nsight</a>.</p>
<h3>Env-dependent metrics</h3>
<p>TODO</p>
<h3>Traditional metrics</h3>
<p>As well as the above, I also plot some other things out of habit</p>
<ul>
<li><p><strong>Reward per trajectory</strong>: should increase dramatically at the start of training. This is, usually, what you care about. Unfortunately it's incredibly noisy and does little to localise errors. Closely related is the <strong>reward per step</strong>, which is typically what you care about in infinite environments.</p>
</li>
<li><p><strong>Mean value</strong>: is (if your value network is working well) a less-noisy proxy for the reward per trajectory. If your trajectories are particularly long compared to your reward discount factor however, this can be dramatically different from the reward per trajectory.</p>
</li>
<li><p><strong>Policy and value losses</strong>: should fall dramatically at the start of training, then level out.</p>
</li>
</ul>
<h1>Why is debugging RL so hard?</h1>
<p>As with many 'failures', the overall pain <a href="https://en.wikipedia.org/wiki/Swiss_cheese_model">is a product of many small pains</a>. Many of these pains are shared by other kinds of systems, but reinforcement learning is rare in having them all in one place.</p>
<h3>Failure is hidden</h3>
<p><strong>Everything has to work for anything to work</strong>:
<strong>Performance is noisy</strong>:</p>
<h3>Simplifying is hard</h3>
<p><strong>There're no good interfaces</strong>:
<strong>There are no good black boxes</strong>:</p>
<h3>You are bad at writing RL systems</h3>
<p><strong>Your intuition sucks</strong>:
<strong>Your expectations suck</strong>:</p>
<h3>Everyone else is bad at writing RL systems</h3>
<p><strong>The community is young</strong>:
<strong>The community has other priorities</strong>:</p>
<h1>A Broad Framework</h1>
<ul>
<li>Focus your efforts</li>
<li>Localise your errors</li>
<li>Maximise detection rate</li>
</ul>
<h1>notes-blogposts</h1>

      </div>
      <div class="column">
        <small>2021/01/01</small>
      </div>
    </div>


    <div>
      <div id="footer" class="column">
        <a href="https://fontawesome.com/license">icons by dave gandy</a>, theme by <a title="i have never been funny" href="https://color-hex.org/color/6d2e98">#6d2e98</a>
      </div>
    </div>
  </body>

</html>